# ğŸ¤– Chat IA Generativa com Azure OpenAI + Streamlit

AplicaÃ§Ã£o web desenvolvida em **Python + Streamlit** que implementa um chat interativo utilizando modelos hospedados no **Azure OpenAI**, com suporte a streaming de respostas em tempo real.

O projeto demonstra:

* IntegraÃ§Ã£o com Azure OpenAI via SDK oficial `openai>=1.x`
* Uso de modelos modernos (ex: `gpt-5.x`)
* Tratamento de erros da API
* Gerenciamento de estado de conversa com `st.session_state`
* Interface organizada com sidebar de configuraÃ§Ãµes
* Streaming de respostas token a token

---

# ğŸ“Œ Objetivo do Projeto

Criar um **chat educacional interativo**, onde o modelo assume o papel de uma professora tÃ©cnica de tecnologia, respondendo:

* De forma clara e didÃ¡tica
* Em portuguÃªs do Brasil
* Com exemplos prÃ¡ticos

O projeto tambÃ©m serve como base para:

* Apps corporativos com Azure OpenAI
* Assistentes educacionais
* Prototipagem de IA generativa

---

# ğŸ—ï¸ Arquitetura da AplicaÃ§Ã£o

## ğŸ”¹ Frontend

* **Streamlit**
* Layout em duas Ã¡reas:

  * Sidebar â†’ ConfiguraÃ§Ãµes e status
  * Ãrea principal â†’ HistÃ³rico + input do chat

## ğŸ”¹ Backend

* SDK `openai` versÃ£o 1.x
* Cliente `AzureOpenAI`
* ComunicaÃ§Ã£o via endpoint configurado no Azure
* Streaming via `chat.completions.create(..., stream=True)`

---

# âš™ï¸ ConfiguraÃ§Ã£o de Ambiente

## 1ï¸âƒ£ VariÃ¡veis de Ambiente (.env)

```env
AZURE_OPENAI_ENDPOINT=https://SEU-ENDPOINT.openai.azure.com/
AZURE_OPENAI_KEY=SUA_CHAVE_AQUI
MODEL_DEPLOY_NAME=gpt-5.2-chat
```

---

## 2ï¸âƒ£ DependÃªncias

Recomendado:

```txt
streamlit>=1.38.0
openai>=1.51.0
python-dotenv>=1.0.1
```

InstalaÃ§Ã£o:

```bash
pip install -r requirements.txt
```

---

# ğŸš€ ExecuÃ§Ã£o

```bash
streamlit run app.py
```

---

# ğŸ§  Funcionamento TÃ©cnico

## 1ï¸âƒ£ InicializaÃ§Ã£o do Cliente

Uso do SDK moderno:

```python
from openai import AzureOpenAI

client = AzureOpenAI(
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_KEY,
    api_version="2024-12-01-preview"
)
```

O cliente Ã© cacheado com:

```python
@st.cache_resource
```

Evita recriaÃ§Ã£o a cada interaÃ§Ã£o.

---

## 2ï¸âƒ£ Gerenciamento do HistÃ³rico

O histÃ³rico Ã© armazenado em:

```python
st.session_state.messages
```

Formato padrÃ£o:

```python
{
    "role": "user" | "assistant" | "system",
    "content": "texto"
}
```

O histÃ³rico Ã©:

* Renderizado na Ã¡rea principal
* Enviado completo para a API a cada nova mensagem

---

## 3ï¸âƒ£ Streaming de Resposta

A chamada Ã© feita com:

```python
response = client.chat.completions.create(
    model=MODEL_DEPLOY_NAME,
    messages=messages_for_api,
    max_completion_tokens=max_tokens,
    stream=True
)
```

E processada assim:

```python
for chunk in response:
    if chunk.choices and chunk.choices[0].delta.content:
        content = chunk.choices[0].delta.content
```

Isso permite:

* Resposta token a token
* ExperiÃªncia semelhante ao ChatGPT
* Melhor percepÃ§Ã£o de desempenho

---

# ğŸ§© Problemas Enfrentados e SoluÃ§Ãµes

Durante o desenvolvimento, ocorreram alguns erros importantes.

---

## âŒ 1. Conflito de dependÃªncias (`proxies`)

Erro:

```
Client.init() got an unexpected keyword argument 'proxies'
```

### ğŸ” Causa

Conflito entre:

* `azure-ai-projects`
* versÃµes antigas de `httpx`
* SDK `openai` 1.x

### âœ… SoluÃ§Ã£o

Remover bibliotecas conflitantes e manter apenas:

```txt
openai>=1.51.0
```

---

## âŒ 2. ParÃ¢metro `max_tokens` nÃ£o suportado

Erro:

```
Unsupported parameter: 'max_tokens'
```

### ğŸ” Causa

Modelos modernos (GPT-5.x) nÃ£o usam mais `max_tokens`.

### âœ… CorreÃ§Ã£o

Substituir:

```python
max_tokens=...
```

Por:

```python
max_completion_tokens=...
```

---

## âŒ 3. `temperature` nÃ£o suportada

Erro:

```
Unsupported value: 'temperature'
```

### ğŸ” Causa

Modelos GPT-5.x possuem temperatura fixa (1.0).

### âœ… SoluÃ§Ã£o

Remover:

```python
temperature
top_p
```

Da chamada da API.

---

# ğŸ¨ Estrutura Final de Interface

## Sidebar

* Temperatura (se modelo permitir)
* MÃ¡ximo de tokens
* BotÃ£o limpar conversa
* Status da conexÃ£o
* InformaÃ§Ãµes do modelo

## Ãrea Principal

* HistÃ³rico do chat
* Streaming da resposta
* Input fixo na parte inferior

---

# ğŸ” Tratamento de Erros

O sistema trata:

* 401 â†’ Erro de autenticaÃ§Ã£o
* 404 â†’ Modelo nÃ£o encontrado
* 429 â†’ Rate limit
* Erros de conexÃ£o
* ExibiÃ§Ã£o parcial da mensagem de erro para debugging

---

# ğŸ“Š Boas PrÃ¡ticas Aplicadas

* Uso de variÃ¡veis de ambiente
* SeparaÃ§Ã£o clara entre UI e lÃ³gica de API
* Cache do cliente
* Streaming
* ValidaÃ§Ã£o de cliente antes da chamada
* Reset controlado de sessÃ£o

---

# ğŸ”® PrÃ³ximos Passos PossÃ­veis

* PersistÃªncia de histÃ³rico em banco (SQLite/Postgres)
* Controle de tokens consumidos
* Upload de documentos (RAG)
* AutenticaÃ§Ã£o via Azure Entra ID
* Deploy no Azure App Service
* ContainerizaÃ§Ã£o com Docker

---

Projeto desenvolvido em aula do curso Desenvolvimento de soluÃ§Ãµes em inteligÃªncia artificial - Microsoft AI-102 Senai

